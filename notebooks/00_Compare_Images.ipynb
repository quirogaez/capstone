{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7828c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "def find_and_handle_duplicates(folder_A: str,\n",
    "                               folder_B: str,\n",
    "                               first_only: bool = True,\n",
    "                               threshold: int = 5,\n",
    "                               output_base: str = \"output\"):\n",
    "    \"\"\"\n",
    "    Busca duplicados entre dos carpetas usando perceptual hash (pHash).\n",
    "    \n",
    "    Parámetros:\n",
    "    - folder_A, folder_B: rutas a los dos datasets\n",
    "    - first_only: \n",
    "        * True  → retorna la primera pareja duplicada (fname_A, fname_B)\n",
    "        * False → organiza archivos en output_base:\n",
    "            - duplicados/…  (subcarpeta por pareja)\n",
    "            - new_dataset/dataset_A/ (únicos de A)\n",
    "            - new_dataset/dataset_B/ (únicos de B)\n",
    "    - threshold: distancia máxima de Hamming para considerar “duplicado”\n",
    "    - output_base: carpeta raíz donde se creará la salida\n",
    "    \n",
    "    Retorna (solo si first_only=True):\n",
    "      (fname_A, fname_B, dist)\n",
    "    \"\"\"\n",
    "    # 1) Construir índice de hashes para cada carpeta\n",
    "    def build_index(folder):\n",
    "        idx = {}\n",
    "        for fn in os.listdir(folder):\n",
    "            path = os.path.join(folder, fn)\n",
    "            try:\n",
    "                idx[fn] = imagehash.phash(Image.open(path))\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipping {fn}: {e}\")\n",
    "        return idx\n",
    "\n",
    "    hash_A = build_index(folder_A)\n",
    "    hash_B = build_index(folder_B)\n",
    "\n",
    "    # 2) Si solo queremos la primera pareja, devolvemos y salimos\n",
    "    for fa, ha in hash_A.items():\n",
    "        for fb, hb in hash_B.items():\n",
    "            dist = abs(ha - hb)\n",
    "            if dist <= threshold:\n",
    "                if first_only:\n",
    "                    return fa, fb, dist\n",
    "\n",
    "    if first_only:\n",
    "        return None  # no se encontró ninguna pareja\n",
    "\n",
    "    # 3) Si vamos a organizar todo:\n",
    "    #    - crear carpetas de salida\n",
    "    dup_base = os.path.join(output_base, \"duplicados\")\n",
    "    uniq_base = os.path.join(output_base, \"new_dataset\")\n",
    "    os.makedirs(dup_base, exist_ok=True)\n",
    "    os.makedirs(os.path.join(uniq_base, \"dataset_A\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(uniq_base, \"dataset_B\"), exist_ok=True)\n",
    "\n",
    "    # 4) detectar todas las parejas duplicadas\n",
    "    duplicated_A = set()\n",
    "    duplicated_B = set()\n",
    "    pairings = []\n",
    "    for fa, ha in hash_A.items():\n",
    "        for fb, hb in hash_B.items():\n",
    "            if abs(ha - hb) <= threshold:\n",
    "                pairings.append((fa, fb))\n",
    "                duplicated_A.add(fa)\n",
    "                duplicated_B.add(fb)\n",
    "\n",
    "    # 5) copiar duplicados en subcarpetas numeradas\n",
    "    for i, (fa, fb) in enumerate(pairings, start=1):\n",
    "        sub = os.path.join(dup_base, f\"pair_{i}\")\n",
    "        os.makedirs(sub, exist_ok=True)\n",
    "        shutil.copy2(os.path.join(folder_A, fa), sub)\n",
    "        shutil.copy2(os.path.join(folder_B, fb), sub)\n",
    "\n",
    "    # 6) copiar únicos restantes\n",
    "    for fa in hash_A:\n",
    "        if fa not in duplicated_A:\n",
    "            shutil.copy2(os.path.join(folder_A, fa),\n",
    "                         os.path.join(uniq_base, \"dataset_A\", fa))\n",
    "    for fb in hash_B:\n",
    "        if fb not in duplicated_B:\n",
    "            shutil.copy2(os.path.join(folder_B, fb),\n",
    "                         os.path.join(uniq_base, \"dataset_B\", fb))\n",
    "\n",
    "    print(f\"✅ Copiadas {len(pairings)} parejas duplicadas en '{dup_base}'\")\n",
    "    print(f\"✅ Imágenes únicas de A en '{os.path.join(uniq_base, 'dataset_A')}'\")\n",
    "    print(f\"✅ Imágenes únicas de B en '{os.path.join(uniq_base, 'dataset_B')}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc9aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera pareja duplicada: NORMAL-1003233-0001.jpeg IM-0734-0001.jpeg dist= 0\n",
      "✅ Copiadas 1651 parejas duplicadas en 'resultados\\duplicados'\n",
      "✅ Imágenes únicas de A en 'resultados\\new_dataset\\dataset_A'\n",
      "✅ Imágenes únicas de B en 'resultados\\new_dataset\\dataset_B'\n"
     ]
    }
   ],
   "source": [
    "# 1) Solo la primera pareja duplicada\n",
    "res = find_and_handle_duplicates(\"CellData/chest_xray/train/NORMAL\", \"Data/chest_xray/train/NORMAL\", first_only=True)\n",
    "if res:\n",
    "    fa, fb, dist = res\n",
    "    print(\"Primera pareja duplicada:\", fa, fb, \"dist=\", dist)\n",
    "else:\n",
    "    print(\"No se encontró duplicado\")\n",
    "\n",
    "# 2) Organizar TODO en subcarpetas\n",
    "find_and_handle_duplicates(\"CellData/chest_xray/train/NORMAL\", \"Data/chest_xray/train/NORMAL\", first_only=False, threshold=5, output_base=\"resultados-neumonia\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac434d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera pareja duplicada: BACTERIA-1008087-0001.jpeg person986_bacteria_2913.jpeg dist= 0\n",
      "✅ Copiadas 3967 parejas duplicadas en 'resultados-neumonia\\duplicados'\n",
      "✅ Imágenes únicas de A en 'resultados-neumonia\\new_dataset\\dataset_A'\n",
      "✅ Imágenes únicas de B en 'resultados-neumonia\\new_dataset\\dataset_B'\n"
     ]
    }
   ],
   "source": [
    "# 1) Solo la primera pareja duplicada\n",
    "res = find_and_handle_duplicates(\"CellData/chest_xray/train/PNEUMONIA\", \"Data/chest_xray/train/PNEUMONIA\", first_only=True)\n",
    "if res:\n",
    "    fa, fb, dist = res\n",
    "    print(\"Primera pareja duplicada:\", fa, fb, \"dist=\", dist)\n",
    "else:\n",
    "    print(\"No se encontró duplicado\")\n",
    "\n",
    "# 2) Organizar TODO en subcarpetas\n",
    "find_and_handle_duplicates(\"CellData/chest_xray/train/PNEUMONIA\", \"Data/chest_xray/train/PNEUMONIA\", first_only=False, threshold=5, output_base=\"resultados-neumonia\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
