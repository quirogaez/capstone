{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37233328",
   "metadata": {
    "id": "37233328"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/quirogaez/capstone/blob/main/notebooks/05_TRAINING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b932e5b",
   "metadata": {
    "id": "2b932e5b"
   },
   "outputs": [],
   "source": [
    "# 01 – Imports y configuración global\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.applications import DenseNet121, EfficientNetB4\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preproc\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preproc\n",
    "\n",
    "# reproducibilidad\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# mixed precision (si GPU lo soporta)\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision': True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MgWrhUivgesd",
   "metadata": {
    "id": "MgWrhUivgesd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "auKs3S8Jglh3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auKs3S8Jglh3",
    "outputId": "7dd8ea0e-6cb2-4b2b-fae7-7649f9d4601b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "IaMLhHzGglbT",
   "metadata": {
    "id": "IaMLhHzGglbT"
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hquPZRI-i_Ry",
   "metadata": {
    "id": "hquPZRI-i_Ry"
   },
   "outputs": [],
   "source": [
    "#!rm -rf /content/my_dir/hyperparam_multiclass SOLO USAR AL REINICIAR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o-oDLG4-_-cJ",
   "metadata": {
    "id": "o-oDLG4-_-cJ"
   },
   "source": [
    "# Estructura general\n",
    "\n",
    "1. **Parámetros y rutas**  \n",
    "2. **Función de carga de backbones**  \n",
    "3. **Definición de `build_model()`**  \n",
    "4. **Carga de los datasets con `tf.data`**  \n",
    "5. **Instanciación y ejecución de Keras Tuner**  \n",
    "\n",
    "---\n",
    "\n",
    "## 1) Parámetros y rutas\n",
    "\n",
    "- **`TRAIN_DIR`** y **`VAL_DIR`**: rutas a las carpetas de entrenamiento y validación.  \n",
    "- **`IMG_SIZE`**: tamaño al que redimensionar todas las imágenes (224×224).  \n",
    "- **`BATCH`**: tamaño de lote para el entrenamiento (32).  \n",
    "- **`AUTOTUNE`**: para acelerar la carga de datos con prefetching automático.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Función `load_backbone(name, input_shape)`\n",
    "\n",
    "Carga un **backbone pre-entrenado** de ImageNet y su función de preprocesado:\n",
    "\n",
    "| `name`         | Modelo base           | Preprocesado                            |\n",
    "|---------------|-----------------------|-----------------------------------------|\n",
    "| `\"densenet\"`  | `DenseNet121`         | `keras.applications.densenet.preprocess_input`  |\n",
    "| `\"effnet_b4\"` | `EfficientNetB4`      | `keras.applications.efficientnet.preprocess_input` |\n",
    "\n",
    "- **Congelamos** (`trainable=False`) todas las capas del backbone para usarlo como extractor de características.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Función `build_model(hp)`\n",
    "\n",
    "Define el **espacio de búsqueda** y construye un modelo Keras:\n",
    "\n",
    "1. **Selección de backbone**  \n",
    "   ```python\n",
    "   hp.Choice('backbone', ['densenet','effnet_b4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sQwydq37G35b",
   "metadata": {
    "id": "sQwydq37G35b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hxDHfSHDgfR7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxDHfSHDgfR7",
    "outputId": "840dd478-90be-48ea-d646-f6122f8ca9c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 02m 37s]\n",
      "val_acc: 0.5454545617103577\n",
      "\n",
      "Best val_acc So Far: 0.5909090638160706\n",
      "Total elapsed time: 01h 55m 18s\n",
      "Results summary\n",
      "Results in my_dir/hyperparam_multiclass\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_acc\", direction=\"max\")\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.30000000000000004\n",
      "lr: 0.0006977759410136506\n",
      "Score: 0.5909090638160706\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.0\n",
      "lr: 1.3348402960430868e-05\n",
      "Score: 0.5909090638160706\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.4\n",
      "lr: 0.00010200445184344612\n",
      "Score: 0.5909090638160706\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.0\n",
      "lr: 2.4026541531091054e-05\n",
      "Score: 0.5909090638160706\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.1\n",
      "lr: 8.47434662661372e-05\n",
      "Score: 0.5909090638160706\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.4\n",
      "lr: 7.191260845067741e-05\n",
      "Score: 0.5909090638160706\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.30000000000000004\n",
      "lr: 0.00011233586048787273\n",
      "Score: 0.5454545617103577\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.0\n",
      "lr: 0.0004376484537751993\n",
      "Score: 0.5454545617103577\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "backbone: effnet_b4\n",
      "dropout: 0.30000000000000004\n",
      "lr: 0.00034381921359473677\n",
      "Score: 0.5454545617103577\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "backbone: densenet\n",
      "dropout: 0.30000000000000004\n",
      "lr: 4.668234933893038e-05\n",
      "Score: 0.5454545617103577\n",
      "\n",
      ">>> Mejores hiperparámetros encontrados:\n",
      "    backbone: densenet\n",
      "    dropout: 0.30000000000000004\n",
      "    lr: 0.0006977759410136506\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "hyperparam_multiclass.py\n",
    "\n",
    "Búsqueda de hiperparámetros para clasificación multiclas de imágenes\n",
    "(3 clases: NORMAL, PNEUMONIA-BACTERIAL, PNEUMONIA-VIRAL).\n",
    "\n",
    "Estructura:\n",
    " 1) Carga de datos con tf.data\n",
    " 2) Definición de función de carga de backbones\n",
    " 3) Definición de build_model() para Keras Tuner\n",
    " 4) Instanciación de BayesianOptimization\n",
    " 5) Ejecución de tuner.search() + análisis de resultados\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import DenseNet121, EfficientNetB4\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Si quieres usar ViT de transformers, descomenta:\n",
    "# from transformers import ViTConfig, TFViTModel\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Parámetros y rutas\n",
    "# -----------------------------------------------------------------------------\n",
    "TRAIN_DIR = \"/content/drive/MyDrive/Capstone/data/multiclass/train\"\n",
    "VAL_DIR   = \"/content/drive/MyDrive/Capstone/data/multiclass/val\"\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH    = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Función para cargar distintos backbones + preprocesado\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_backbone(name, input_shape=(*IMG_SIZE, 3)):\n",
    "    \"\"\"\n",
    "    name: 'densenet', 'effnet_b4' o 'vit_b16'\n",
    "    devuelve: (modelo_base_sin_top, función_preprocesado)\n",
    "    \"\"\"\n",
    "    if name == 'densenet':\n",
    "        base = DenseNet121(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "        preproc = keras.applications.densenet.preprocess_input\n",
    "\n",
    "    elif name == 'effnet_b4':\n",
    "        base = EfficientNetB4(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "        preproc = keras.applications.efficientnet.preprocess_input\n",
    "\n",
    "    # elif name == 'vit_b16':\n",
    "    #     # Ejemplo con HuggingFace Vision Transformer\n",
    "    #     config = ViTConfig(\n",
    "    #         image_size=224,\n",
    "    #         num_labels=0,\n",
    "    #         hidden_size=768,\n",
    "    #         num_hidden_layers=12,\n",
    "    #         num_attention_heads=12,\n",
    "    #         patch_size=16\n",
    "    #     )\n",
    "    #     base = TFViTModel.from_pretrained(\n",
    "    #         'google/vit-base-patch16-224-in21k',\n",
    "    #         config=config\n",
    "    #     )\n",
    "    #     preproc = lambda x: tf.cast(x, tf.float32) / 255.0\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Backbone desconocido: {name}\")\n",
    "\n",
    "    base.trainable = False\n",
    "    return base, preproc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) build_model() para Keras Tuner\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_model(hp):\n",
    "    # 3.1) Elige backbone\n",
    "    backbone_name = hp.Choice('backbone', ['densenet', 'effnet_b4'])\n",
    "    base, preproc_fn = load_backbone(backbone_name)\n",
    "\n",
    "    # 3.2) Input + preprocesado + backbone\n",
    "    inputs = keras.Input(shape=(*IMG_SIZE,3), name='input_image')\n",
    "    x = preproc_fn(inputs)\n",
    "    x = base(x, training=False)\n",
    "\n",
    "    # 3.3) Pooling global + Dropout (hiperparámetro)\n",
    "    x = layers.GlobalAveragePooling2D(name='gap')(x)\n",
    "    x = layers.Dropout(\n",
    "        hp.Float('dropout', 0.0, 0.5, step=0.1),\n",
    "        name='dropout'\n",
    "    )(x)\n",
    "\n",
    "    # 3.4) Capa de salida multiclas (3 neuronas + softmax)\n",
    "    outputs = layers.Dense(3, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name='multiclass_model')\n",
    "\n",
    "    # 3.5) Compilar con loss y métricas adecuadas\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Float('lr', 1e-5, 1e-3, sampling='log'),\n",
    "            name='adam_lr'\n",
    "        ),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name='acc'),\n",
    "            # Si quieres AUC multiclas:\n",
    "            # keras.metrics.AUC(name='auc', multi_label=True, num_labels=3)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Carga de datasets con tf.data\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_datasets():\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        VAL_DIR,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    # Prefetch para rendimiento\n",
    "    train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "    val_ds   = val_ds.prefetch(AUTOTUNE)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Instanciar y ejecutar Keras Tuner\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_tuner():\n",
    "    train_ds, val_ds = get_datasets()\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_model,\n",
    "        objective='val_acc',      # usamos accuracy; si cambias a AUC, pon 'val_auc'\n",
    "        max_trials=20,\n",
    "        directory='my_dir',\n",
    "        project_name='hyperparam_multiclass'\n",
    "    )\n",
    "\n",
    "    # Early stopping sobre la métrica de validación\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=10,\n",
    "        callbacks=[stop_early]\n",
    "    )\n",
    "\n",
    "    # 6) Resumen de resultados\n",
    "    tuner.results_summary()\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"\\n>>> Mejores hiperparámetros encontrados:\")\n",
    "    for param, val in best_hp.values.items():\n",
    "        print(f\"    {param}: {val}\")\n",
    "\n",
    "#run_tuner()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "FkroFjxwG_Ib",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkroFjxwG_Ib",
    "outputId": "d27ebc4e-1bb9-4d02-e1a5-6a4c7c3c4784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7226d794-7e9e-42c1-b3e3-044f9e6a4681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test GPU con TensorFlow ===\n",
      "GPUs disponibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 01:23:36.125315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-01 01:23:36.129381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-01 01:23:36.129803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def configure_gpu():\n",
    "    # Permite que TF crezca la memoria en lugar de reservarla toda de golpe\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if not gpus:\n",
    "        print(\"⚠️  No GPUs detectadas. Saliendo.\")\n",
    "        exit(1)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"GPUs disponibles:\", gpus)\n",
    "\n",
    "print(\"=== Test GPU con TensorFlow ===\")\n",
    "configure_gpu()\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0127ff1c-0fd8-4d6c-83d9-cd409df5d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf  my_dir/chexnet_hyperparam_refined/tuner0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1Onk3ZhkFSwW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Onk3ZhkFSwW",
    "outputId": "3742ba3c-c2fb-46c0-d949-a117c0e34495",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 05s]\n",
      "val_acc: 0.3181818127632141\n",
      "\n",
      "Best val_acc So Far: 0.5\n",
      "Total elapsed time: 00h 26m 39s\n",
      "Results summary\n",
      "Results in my_dir/chexnet_hyperparam_refined\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_acc\", direction=\"max\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.0\n",
      "learning_rate: 1.0779892690656582e-05\n",
      "optimizer: adamw\n",
      "Score: 0.5\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.0\n",
      "learning_rate: 1.2100818369205357e-05\n",
      "optimizer: adamw\n",
      "Score: 0.5\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.0\n",
      "learning_rate: 1.1532423223506732e-05\n",
      "optimizer: adamw\n",
      "Score: 0.5\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.0\n",
      "learning_rate: 0.00045454167020149807\n",
      "optimizer: adam\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 11 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.2\n",
      "learning_rate: 0.0005728965578955454\n",
      "optimizer: adamw\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.0\n",
      "learning_rate: 1e-05\n",
      "optimizer: adamw\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.05\n",
      "learning_rate: 0.00025510751612212846\n",
      "optimizer: adamw\n",
      "Score: 0.40909090638160706\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.1\n",
      "learning_rate: 0.0002578387751080839\n",
      "optimizer: adamw\n",
      "Score: 0.40909090638160706\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.15000000000000002\n",
      "learning_rate: 0.0009950736716951277\n",
      "optimizer: adamw\n",
      "Score: 0.40909090638160706\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "dropout: 0.0\n",
      "learning_rate: 5.229933681657186e-05\n",
      "optimizer: adam\n",
      "Score: 0.40909090638160706\n",
      "\n",
      ">>> Mejores hiperparámetros refinados con CheXNet:\n",
      "    dropout: 0.0\n",
      "    learning_rate: 1.0779892690656582e-05\n",
      "    optimizer: adamw\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "hyperparam_multiclass_with_chexnet_refined.py\n",
    "\n",
    "Versión refinada para usar tu CheXNet SavedModel legacy en Keras 3,\n",
    "con toggle de rutas local vs Google Drive, data augmentation,\n",
    "normalización tras CheXNet (sin pooling) y búsqueda de hiperparámetros centrada.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SWITCH de entorno: True para Drive, False para local (cwd)\n",
    "# -----------------------------------------------------------------------------\n",
    "USE_DRIVE = False\n",
    "if USE_DRIVE:\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Capstone\"\n",
    "else:\n",
    "    BASE_PATH = \"/workspace\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Directorio de tu SavedModel de CheXNet\n",
    "# -----------------------------------------------------------------------------\n",
    "CHEXNET_SM_DIR = os.path.join(BASE_PATH, \"saved_models\", \"chexnet\")\n",
    "if not os.path.isdir(CHEXNET_SM_DIR):\n",
    "    raise FileNotFoundError(f\"No existe el SavedModel en {CHEXNET_SM_DIR}\")\n",
    "\n",
    "# Asegurar carpeta variables/\n",
    "vars_folder = os.path.join(CHEXNET_SM_DIR, \"variables\")\n",
    "if not os.path.isdir(vars_folder):\n",
    "    os.makedirs(vars_folder, exist_ok=True)\n",
    "    for fn in os.listdir(CHEXNET_SM_DIR):\n",
    "        if fn.startswith(\"variables.\") and os.path.isfile(os.path.join(CHEXNET_SM_DIR, fn)):\n",
    "            shutil.move(\n",
    "                os.path.join(CHEXNET_SM_DIR, fn),\n",
    "                os.path.join(vars_folder, fn)\n",
    "            )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Cargo el módulo SavedModel y extraigo la función de inferencia\n",
    "# -----------------------------------------------------------------------------\n",
    "chexnet_module = tf.saved_model.load(CHEXNET_SM_DIR)\n",
    "infer_fn        = chexnet_module.signatures[\"serving_default\"]\n",
    "out_key         = list(infer_fn.structured_outputs.keys())[0]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Parámetros y rutas de datos\n",
    "# -----------------------------------------------------------------------------\n",
    "TRAIN_DIR = os.path.join(BASE_PATH, \"multiclass\", \"train\")\n",
    "VAL_DIR   = os.path.join(BASE_PATH, \"multiclass\", \"val\")\n",
    "IMG_SIZE  = (224, 224)\n",
    "BATCH     = 32\n",
    "AUTOTUNE  = tf.data.AUTOTUNE\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Creación de datasets \n",
    "# -----------------------------------------------------------------------------\n",
    "def get_datasets(batch_size):\n",
    "    augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomZoom(0.05),\n",
    "        layers.RandomBrightness(factor=0.1),\n",
    "    ], name=\"augmentation\")\n",
    "\n",
    "    def preprocess_train(images, labels):\n",
    "        images = augmentation(images)\n",
    "        images = keras.applications.densenet.preprocess_input(images)\n",
    "        return images, labels\n",
    "\n",
    "    def preprocess_val(images, labels):\n",
    "        images = keras.applications.densenet.preprocess_input(images)\n",
    "        return images, labels\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    ).map(preprocess_train, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        VAL_DIR,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    ).map(preprocess_val, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return train_ds.prefetch(AUTOTUNE), val_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) build_model() para Keras Tuner con normalización (sin pooling)\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_model(hp):\n",
    "    inputs = keras.Input(shape=(*IMG_SIZE, 3), name=\"input_image\")\n",
    "    x = keras.applications.densenet.preprocess_input(inputs)\n",
    "\n",
    "    # Llamada al bloque CheXNet\n",
    "    def call_chexnet(tensor):\n",
    "        return infer_fn(tensor)[out_key]\n",
    "    x = layers.Lambda(call_chexnet, name=\"chexnet_block\")(x)\n",
    "\n",
    "    # Normalización de la salida 2D (batch, 14)\n",
    "    x = layers.BatchNormalization(name=\"bn_chexnet\")(x)\n",
    "\n",
    "    # Dropout ajustable sobre ese vector de 14 features\n",
    "    x = layers.Dropout(\n",
    "        hp.Float(\"dropout\", min_value=0.0, max_value=0.2, step=0.05),\n",
    "        name=\"dropout\"\n",
    "    )(x)\n",
    "\n",
    "    outputs = layers.Dense(3, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"chexnet_multiclass\")\n",
    "\n",
    "    # Optimizer y learning rate\n",
    "    lr = hp.Float(\"learning_rate\", 1e-5, 1e-3, sampling=\"log\")\n",
    "    opt_choice = hp.Choice(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "    if opt_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.experimental.AdamW(learning_rate=lr)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Ejecución del tuner con callbacks mejorados\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_tuner():\n",
    "    train_ds, val_ds = get_datasets(batch_size=BATCH)\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_model,\n",
    "        objective=\"val_acc\",\n",
    "        max_trials=20,\n",
    "        directory=\"my_dir\",\n",
    "        project_name=\"chexnet_hyperparam_refined\"\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_acc\", patience=5, mode=\"max\", restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    tuner.search(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=20,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    tuner.results_summary()\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"\\n>>> Mejores hiperparámetros refinados con CheXNet:\")\n",
    "    for param, val in best_hp.values.items():\n",
    "        print(f\"    {param}: {val}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tuner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0164132e-22d4-48ec-a52e-d4c6e6421ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 02m 24s]\n",
      "val_acc: 0.3636363744735718\n",
      "\n",
      "Best val_acc So Far: 0.5\n",
      "Total elapsed time: 00h 58m 57s\n",
      "Results summary\n",
      "Results in /workspace/my_dir/chexnet_hyperparam_final\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_acc\", direction=\"max\")\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "dense_units: 128\n",
      "dropout: 0.1\n",
      "learning_rate: 2.8507137885985768e-05\n",
      "optimizer: adam\n",
      "Score: 0.5\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "dense_units: 128\n",
      "dropout: 0.1\n",
      "learning_rate: 3.381099566791077e-05\n",
      "optimizer: adam\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "dense_units: 128\n",
      "dropout: 0.0\n",
      "learning_rate: 2.117018483773315e-06\n",
      "optimizer: adamw\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 14 summary\n",
      "Hyperparameters:\n",
      "dense_units: 128\n",
      "dropout: 0.1\n",
      "learning_rate: 2.975684481471653e-05\n",
      "optimizer: adam\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "dense_units: 128\n",
      "dropout: 0.1\n",
      "learning_rate: 6.729078586838823e-05\n",
      "optimizer: adam\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 21 summary\n",
      "Hyperparameters:\n",
      "dense_units: 128\n",
      "dropout: 0.15000000000000002\n",
      "learning_rate: 3.5254521609859955e-06\n",
      "optimizer: adamw\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 27 summary\n",
      "Hyperparameters:\n",
      "dense_units: 64\n",
      "dropout: 0.0\n",
      "learning_rate: 7.062762353087982e-06\n",
      "optimizer: adamw\n",
      "Score: 0.4545454680919647\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "dense_units: 64\n",
      "dropout: 0.15000000000000002\n",
      "learning_rate: 5.305163878204072e-05\n",
      "optimizer: adam\n",
      "Score: 0.40909090638160706\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "dense_units: 64\n",
      "dropout: 0.0\n",
      "learning_rate: 8.060771879266742e-06\n",
      "optimizer: adam\n",
      "Score: 0.40909090638160706\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "dense_units: 128\n",
      "dropout: 0.15000000000000002\n",
      "learning_rate: 1.4273272133361508e-05\n",
      "optimizer: adam\n",
      "Score: 0.40909090638160706\n",
      "\n",
      ">>> Mejores hiperparámetros finales:\n",
      "    dense_units: 128\n",
      "    dropout: 0.1\n",
      "    learning_rate: 2.8507137885985768e-05\n",
      "    optimizer: adam\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "hyperparam_multiclass_with_chexnet_final.py\n",
    "\n",
    "Versión estable:\n",
    " - Toggle de rutas local vs Drive\n",
    " - Data augmentation suave\n",
    " - Normalización tras CheXNet\n",
    " - Head con capa densa intermedia + Dropout\n",
    " - Búsqueda de hiperparámetros: lr, optimizer, dropout, dense_units\n",
    " - Callbacks: EarlyStopping + ReduceLROnPlateau\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SWITCH de entorno: True para Drive, False para local\n",
    "# -----------------------------------------------------------------------------\n",
    "USE_DRIVE = False\n",
    "if USE_DRIVE:\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Capstone\"\n",
    "else:\n",
    "    BASE_PATH = \"/workspace\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# (Opcional) Limpia resultados previos del tuner\n",
    "# -----------------------------------------------------------------------------\n",
    "RESULTS_DIR = os.path.join(BASE_PATH, \"my_dir\", \"chexnet_hyperparam_final\")\n",
    "if os.path.isdir(RESULTS_DIR):\n",
    "    shutil.rmtree(RESULTS_DIR)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Carga de tu SavedModel de CheXNet y firma de inferencia\n",
    "# -----------------------------------------------------------------------------\n",
    "CHEXNET_SM_DIR = os.path.join(BASE_PATH, \"saved_models\", \"chexnet\")\n",
    "if not os.path.isdir(CHEXNET_SM_DIR):\n",
    "    raise FileNotFoundError(f\"No existe el SavedModel en {CHEXNET_SM_DIR}\")\n",
    "\n",
    "chexnet_module = tf.saved_model.load(CHEXNET_SM_DIR)\n",
    "infer_fn        = chexnet_module.signatures[\"serving_default\"]\n",
    "out_key         = list(infer_fn.structured_outputs.keys())[0]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Parámetros y rutas de datos\n",
    "# -----------------------------------------------------------------------------\n",
    "TRAIN_DIR = os.path.join(BASE_PATH, \"multiclass\", \"train\")\n",
    "VAL_DIR   = os.path.join(BASE_PATH, \"multiclass\", \"val\")\n",
    "IMG_SIZE  = (224, 224)\n",
    "BATCH     = 32\n",
    "AUTOTUNE  = tf.data.AUTOTUNE\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Data augmentation suave y pipeline tf.data\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_datasets(batch_size):\n",
    "    aug = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.02),\n",
    "        layers.RandomZoom(0.02),\n",
    "        layers.RandomBrightness(factor=0.05),\n",
    "    ], name=\"augmentation\")\n",
    "\n",
    "    def preprocess_train(images, labels):\n",
    "        images = aug(images)\n",
    "        images = keras.applications.densenet.preprocess_input(images)\n",
    "        return images, labels\n",
    "\n",
    "    def preprocess_val(images, labels):\n",
    "        images = keras.applications.densenet.preprocess_input(images)\n",
    "        return images, labels\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR, labels=\"inferred\", label_mode=\"categorical\",\n",
    "        image_size=IMG_SIZE, batch_size=batch_size, shuffle=True\n",
    "    ).map(preprocess_train, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        VAL_DIR, labels=\"inferred\", label_mode=\"categorical\",\n",
    "        image_size=IMG_SIZE, batch_size=batch_size, shuffle=False\n",
    "    ).map(preprocess_val, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return train_ds.prefetch(AUTOTUNE), val_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Definición del modelo para Keras Tuner\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_model(hp):\n",
    "    inputs = keras.Input(shape=(*IMG_SIZE, 3), name=\"input_image\")\n",
    "    x = keras.applications.densenet.preprocess_input(inputs)\n",
    "\n",
    "    # Invoca al SavedModel CheXNet\n",
    "    def call_chexnet(t):\n",
    "        return infer_fn(t)[out_key]\n",
    "    x = layers.Lambda(call_chexnet, name=\"chexnet_block\")(x)\n",
    "\n",
    "    # Normalización de la salida (batch, 14)\n",
    "    x = layers.BatchNormalization(name=\"bn_chexnet\")(x)\n",
    "\n",
    "    # Capa oculta intermedia\n",
    "    x = layers.Dense(\n",
    "        units=hp.Choice(\"dense_units\", [64, 128]),\n",
    "        activation=\"relu\",\n",
    "        name=\"hidden_dense\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(\n",
    "        rate=hp.Float(\"dropout\", 0.0, 0.2, step=0.05),\n",
    "        name=\"dropout\"\n",
    "    )(x)\n",
    "\n",
    "    outputs = layers.Dense(3, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"chexnet_multiclass_final\")\n",
    "\n",
    "    # Optimizer y learning rate\n",
    "    lr = hp.Float(\"learning_rate\", 1e-6, 1e-4, sampling=\"log\")\n",
    "    opt = hp.Choice(\"optimizer\", [\"adamw\", \"adam\"])\n",
    "    if opt == \"adamw\":\n",
    "        optimizer = keras.optimizers.experimental.AdamW(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Ejecutar el tuner con callbacks avanzados\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_tuner():\n",
    "    train_ds, val_ds = get_datasets(batch_size=BATCH)\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        hypermodel=build_model,\n",
    "        objective=\"val_acc\",\n",
    "        max_trials=30,\n",
    "        directory=os.path.join(BASE_PATH, \"my_dir\"),\n",
    "        project_name=\"chexnet_hyperparam_final\"\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_acc\", patience=7, mode=\"max\", restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    tuner.search(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=25,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    tuner.results_summary()\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"\\n>>> Mejores hiperparámetros finales:\")\n",
    "    for param, val in best_hp.values.items():\n",
    "        print(f\"    {param}: {val}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tuner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c3df3-6a55-4c06-993a-feb3ce096a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
